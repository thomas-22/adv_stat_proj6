@article{Huber2003,
  author = {Huber, Susanne and Palme, Rupert and Zenker, Wolfgang and Möstl, Erich},
  year = {2003},
  month = {04},
  pages = {258-266},
  title = {Non-Invasive Monitoring of the Adrenocortical Response in Red Deer},
  volume = {67},
  journal = {Journal of Wildlife Management},
  doi = {10.2307/3802767}
}

@article{Möstl2002,
author = {Möstl, Erich and Palme, Rupert},
year = {2002},
month = {08},
pages = {67-74},
title = {Hormones as indicators of stress},
volume = {23},
journal = {Domestic animal endocrinology},
doi = {10.1016/S0739-7240(02)00146-7}
}

@article{Palme2019,
author = {Palme, Rupert},
year = {2019},
month = {02},
pages = {229-243.},
title = {Non-invasive measurement of glucocorticoids: Advances and problems},
volume = {199},
journal = {Physiology & Behavior},
doi = {10.1016/j.physbeh.2018.11.021}
}

@article{mgcv,
  title = {Fast stable restricted maximum likelihood and marginal
    likelihood estimation of semiparametric generalized linear
    models},
  journal = {Journal of the Royal Statistical Society (B)},
  volume = {73},
  number = {1},
  pages = {3-36},
  year = {2011},
  author = {S. N. Wood},
}

@article{Vilela2020,
  title = {Physiological {{Stress Reactions}} in {{Red Deer Induced}} by {{Hunting Activities}}},
  author = {Vilela, Sofia and {Alves da Silva}, Ant{\'o}nio and Palme, Rupert and Ruckstuhl, Kathreen E. and Sousa, Jos{\'e} Paulo and Alves, Joana},
  year = {2020},
  month = jun,
  journal = {Animals: an open access journal from MDPI},
  volume = {10},
  number = {6},
  pages = {1003},
  issn = {2076-2615},
  doi = {10.3390/ani10061003}
}

@inproceedings{Chen2016,
  author = {Chen, Tianqi and Guestrin, Carlos},
  title = {XGBoost: A Scalable Tree Boosting System},
  year = {2016},
  isbn = {9781450342322},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/2939672.2939785},
  doi = {10.1145/2939672.2939785},
  abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
  booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages = {785–794},
  numpages = {10},
  keywords = {large-scale machine learning},
  location = {San Francisco, California, USA},
  series = {KDD '16}
}

@article{Bergstra2012,
author = {Bergstra, James and Bengio, Yoshua},
title = {Random search for hyper-parameter optimization},
year = {2012},
issue_date = {3/1/2012},
publisher = {JMLR.org},
volume = {13},
number = {null},
issn = {1532-4435},
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent "High Throughput" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
journal = {J. Mach. Learn. Res.},
month = feb,
pages = {281–305},
numpages = {25},
keywords = {deep learning, global optimization, model selection, neural networks, response surface modeling}
}

@article{Pinero2025,
author = {Piñero, Justin and Jansen, Heiko and Robbins, Charles and Vincent, Ellery and Lafferty, Diana},
year = {2025},
month = {01},
pages = {},
title = {Blood cortisol and faecal cortisol metabolite concentrations following an ACTH challenge in unanaesthetized brown bears ( Ursus arctos )},
volume = {13},
journal = {Conservation Physiology},
doi = {10.1093/conphys/coae093}
}